{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Keras Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Theoritic Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare datasets<br>\n",
    "a. Parse data<br>\n",
    "b. Index variables<br>\n",
    "c. Split train and test<br>\n",
    "2. Add layers<br>\n",
    "a. Layer type and number<br>\n",
    "b. Layer parameters<br>\n",
    "3. Compile and fit the model<br>\n",
    "a. Training data<br>\n",
    "b. Optimizer<br>\n",
    "c. Loss function and Metrics<br>\n",
    "4. Evaluate and predict<br>\n",
    "a. Test data<br>\n",
    "b. Metrics<br>\n",
    "c. Predict new data<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Steps with Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Sequential model (a linear stack of layers)</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(units=64, imput_dim=100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Compile model</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', opotimizer='sgd', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can further configure optimizer<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Training</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed data manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_on_batch(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. Evaluation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5. Prediction</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Models :</b> Sequential(), LSTM()<br>\n",
    "<b>Layers :</b> Dense, dropout, activation, convolutional layer, pooling<br>\n",
    "<b>Compile :</b> Loss, ptimizer<br>\n",
    "<b>fit :</b> train, test, X, Y, lr(learning rate), epochs, convergence criteria, batch size<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary_crossentropy<br>\n",
    "categorical_crossentropy<br>\n",
    "mean_squared_error<br>\n",
    "sparse_categorical_crossentropy<br>\n",
    "mean_absolute_error<br>\n",
    "mean_absolute_percentage_error<br>\n",
    "mean_squared_logarithmic_error<br>\n",
    "squared_hinge<br>\n",
    "hinge<br>\n",
    "categorical_hinge<br>\n",
    "logcosh<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD<br>\n",
    "RMSprop<br>\n",
    "Nadam<br>\n",
    "Adadelta<br>\n",
    "Adam<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Accuracy<br>\n",
    "a. binary_accuracy<br>\n",
    "b. categorical_accuracy<br>\n",
    "c. sparse_categorical_accuracy<br>\n",
    "d. top_k_categorical_accuracy<br>\n",
    "e. sparse_top_k_categorical_accuracy<br>\n",
    "2. Precision<br>\n",
    "3. Recall<br>\n",
    "4. F1 Score<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy', 'f1score', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metric\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='rpsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Model save, load or delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('my_model.h5')\n",
    "del model\n",
    "\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : We can save/load only a model's architecture or weights only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Allowing function call during training\n",
    "* callbacks can be used at different points of training(batch or epochs)\n",
    "* Existing callbacks : Early stopping, weight saving after epoch\n",
    "* Easy to build and implement, called in training function, fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Case Study(customer churm prediction) Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Import all libraries</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Load csv dataset</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Use pandas profiling</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. This will give detailed data audit report<br>\n",
    "b. Dataset information<br>\n",
    "b. Types of variables<br>\n",
    "c. Warnings for zeros, distinct values and missing values<br>\n",
    "d. Variablewise distribution<br>\n",
    "e. Statistics, histogram, common vlaues and extreme values of each variable<br>\n",
    "f. Different correlation metrics<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. Drop unwanted variables</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Look for the important/helpful variables for predicting output<br>\n",
    "b. Drop unwated variables<br>\n",
    "dataset.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5. Get dummies for Categorical variables</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Get dummies for the categorical variables :<br>\n",
    "dataset_new = pd.get_dummies(dataset, ['Geography', 'Gender'], drop_first=True)<br>\n",
    "b. Check dimesions of the dataset :<br>\n",
    "dataset.shape<br>\n",
    "dataset.columns<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6. Separate X and Y variables</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_new[dataset_new.columns.difference(['Exited'])]\n",
    "y = dataset_new['Exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Cross check for X and Y variables<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>7. Split into Train and Test</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Check for dimensions after split for train_x, test_x, train_y, test_y<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>8. Scaling variables(part of feature engineering)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Feature scaling helps in converging faster<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "train_x = sc.fit_transform(x_train)\n",
    "test_x = sc.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>9. Import keras modules and steps</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.models as km\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network = [11, 6, 6, 1] \n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim = 6, init='uniform', activation='relu', input_dim=11)) # init - initialization of betas\n",
    "model.add(Dense(output_dim = 6, init='uniform', activation='relu')) # input_dim taken from previous step\n",
    "model add(Dense(output_dim = 1, init='uniform', activation='sigmoid')) # sigmoid/softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y, batch_size = 10, nb_epoch = 10)\n",
    "# batch_size 10 means if there are 700 observations 700/10=70 times betas will adjust\n",
    "# nb_epoch, no of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "metrics.roc_auc_score(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we get model from somewhere else, we can use this method to see architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensorflow is an open source sw libraryfor numerical computation using dataflow graphs.\n",
    "* Nodes in the graph represent mathematical operations while graph edges represent the multidimensional data arrays(tensors) that flow between them\n",
    "* Python based neural network framework.\n",
    "* Runs highly optimized c++ code for actual calculations.\n",
    "* Higher level APIs on top of tensorflow available, like skflow to fit within Scikit-learn API.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fundamental tensorflow workflow</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with tensorflow, our code will be effectively be divided into two parts:<br>\n",
    "    1. Define a graph which contains our model.\n",
    "    2. Run the graph. Two special cases are:\n",
    "        a. Train the model\n",
    "        b. Test/Predict using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Example :</b><br>\n",
    "<b>Step 1 : Define a computational graph</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.placegolder creates an 'input' node. We must give it value when we runour model.\n",
    "# these can be data we want to learn from or values of hyper-parametersfor out model  \n",
    "a = tf.placeholder(tf.int32, name='input_a')\n",
    "b = tf.placeholder(tf.int32, name='input_b')\n",
    "\n",
    "# tf.add creates an addition node\n",
    "c = tf.add(a, b, name='add')\n",
    "\n",
    "# tf.multiply creates a multiplication node\n",
    "d = tf.multiply(a, b, name='multiply')\n",
    "\n",
    "# tf.subtract creates a subtraction node\n",
    "e = tf.subtract(c, d, name='subtract')\n",
    "\n",
    "# add up results of previous nodes\n",
    "out = tf.add(e, b, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out # it is null as our placeholder doesnot contain any values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2 : Run the graph</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Create a 'feed_dict' dictionary to define input values\n",
    "# Keys to dictionaries are handles to our placeholders\n",
    "# Values to dictionary are values we would like to feed in\n",
    "feed_dict = {a : 4, b : 3}\n",
    "\n",
    "# Execute the graph using 'Session.run()', which takes two parameters:\n",
    "# - 'fetches' Lists which nodes we'd like to receiveas output\n",
    "# - 'feed_dict', feeds in key-value pairs to input to various nodes\n",
    "# In this case, we passin the Tensor 'out'as our valuefor'fetches',\n",
    "# which causes the valuefor out to be computed and returned\n",
    "result = sess.run(out, feed_dict=feed_dict)\n",
    "\n",
    "# print the value of out\n",
    "print(\"(({0}*{1}) - ({0}+{1}) + {1} = {2}\".format(feed_dict[a], feed_dict[b], result))\n",
    "\n",
    "# close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Reg_data.csv')\n",
    "data.head()\n",
    "# country | GDP | Life expectancy\n",
    "# drop country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.drop(['Country'], axis=1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')\n",
    "w = tf.get_variable('weights', initializer=tf.constant(0.0))\n",
    "b = tf.get_variable('bias', initializer=tf.constant(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = w * X + b  # alternate to using tf.add()/tf.multiply()\n",
    "loss = tf.square(Y - Y_predicted, name='loss')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0003).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # train model\n",
    "    for i in range(500):  # for every epoch\n",
    "        total_loss = 0\n",
    "        for x, y  in data1: # for every observation, we are calculating the graph\n",
    "            _, mloss = sess.run([optimizer, loss], feed_dict={X:x, Y:y})\n",
    "            total_loss += mloss\n",
    "            \n",
    "        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
    "    w_out, b_out = sess.run([w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[:,1]*w_out+b_out   # predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[:,0] # actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
